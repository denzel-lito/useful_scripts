{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def last_day_of_previous_month(any_day):\n",
    "    return any_day - timedelta(days=any_day.day)\n",
    "\n",
    "def current_month(any_day):\n",
    "    next_month = any_day.replace(day=28) + timedelta(days=4)\n",
    "    return next_month - timedelta(days=next_month.day)\n",
    "\n",
    "target_df['evt_dt'] = pd.to_datetime(target_df['evt_dt'], format='%Y-%m-%d')\n",
    "dates_df['mnth'] = dates_df['evt_dt'].apply(current_month)\n",
    "\n",
    "def gen_date_list(start_date, end_date):\n",
    "    s_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    e_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "\n",
    "    date_list = []\n",
    "    for i in range( (e_date - s_date).days + 1):\n",
    "        date_list.append(str(s_date + timedelta(days=i)))\n",
    "        \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# список признаков, по которым только одно уникальное значение\n",
    "def single_unique(dataset):\n",
    "    unique_counts = dataset.nunique()\n",
    "    unique_counts = pd.DataFrame(unique_counts).reset_index().rename(columns = {'index': 'feature', 0: 'nunique'})\n",
    "    single_unique = pd.DataFrame(unique_counts[unique_counts['nunique'] <= 1])\n",
    "    single_unique = list(single_unique['feature'])\n",
    "    pd.DataFrame(single_unique).to_csv('dop_files/drop_single.csv', index=False) #############################\n",
    "    return single_unique\n",
    "\n",
    "# удаление полей по бизнес-логике\n",
    "def dropping(dataset):\n",
    "    # удаление полей, заданных в файле \"Приложение2.xlsx\"\n",
    "    on_drop = []\n",
    "    data_on_delete = pd.read_excel('Приложение2.xlsx') \n",
    "    for i in data_on_delete['Атрибут']:\n",
    "        if i in dataset.columns: on_drop.append(i) \n",
    "    #удаление из датафрейма полей ВСП,ОСБ\n",
    "    dataset.drop(on_drop, 1, inplace=True)  \n",
    "    # удаление дат\n",
    "    dates = [i for i in dataset.columns if i.find('_dt') != -1]\n",
    "    dataset.drop(dates, 1, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "# определение типов полей\n",
    "def types(dataset):\n",
    "    cat_features = [i[0] for i in dict(dataset.dtypes).items() if 'obj' in str(i[1])]\n",
    "    float_features = [i[0] for i in dict(dataset.dtypes).items() if 'float' in str(i[1])]\n",
    "    int_features = [i[0] for i in dict(dataset.dtypes).items() if 'int' in str(i[1])]\n",
    "    #date_features = [i[0] for i in dict(dataset.dtypes).items() if 'date' in str(i[1])]\n",
    "    date_features = [i for i in dataset.columns if i.find('_dt') != -1]\n",
    "    # добавление кодов (например код тер.банка) в категориальные признаки\n",
    "    cat_dop = [i for i in dataset.columns if i.find('_cd') != -1]      \n",
    "    for i in cat_dop:\n",
    "        if i not in cat_features: \n",
    "            cat_features.append(i)\n",
    "            if i in float_features: float_features.remove(i)\n",
    "            if i in int_features: int_features.remove(i)\n",
    "    # удаление полей типа даты из категориальных\n",
    "    for i in date_features: \n",
    "        if i in cat_features: cat_features.remove(i)  \n",
    "    # объединение целых и вещественных полей\n",
    "    numeric_features = float_features + int_features\n",
    "    return cat_features, numeric_features, date_features\n",
    "\n",
    "# поиск и изменение аномальных значений\n",
    "def anomalies(dataset, columns):\n",
    "    for i in columns: #numbers / not categories\n",
    "        col = dataset[[i]].sort_values(i)\n",
    "        percent1 = np.percentile(col, 1)\n",
    "        percent99 = np.percentile(col, 99)\n",
    "        dataset[i][dataset[i] > percent99] = percent99\n",
    "        dataset[i][dataset[i] < percent1] = percent1\n",
    "\n",
    "# преобразование признаков для моделей градиентного бустинга\n",
    "def transform_data(dataset, cat_features, numeric_features, date_features, fillna = True):\n",
    " \n",
    "    # преобразование полей типа дата\n",
    "    today = datetime.date.today()\n",
    "    for i in date_features:  #date_features:\n",
    "        dataset[i] = pd.to_datetime(dataset[i], format='%Y-%m-%d')\n",
    "        dataset[i] = dataset[i].apply(lambda x: ((today - x.date()).days) if x is not None else x)\n",
    "    pd.DataFrame(date_features).to_csv('dop_files/fillna_date.csv', index=False, header=False) #############################\n",
    "      \n",
    "    if fillna == True: # заполнять или нет пустые ячейки\n",
    "        # заполнение пропусков для категор. приз-в на -99999\n",
    "        numeric_features = numeric_features + date_features\n",
    "        a = [i for i in numeric_features if dataset[i].count() != dataset.shape[0]]    ############################# \n",
    "        pd.DataFrame(a).to_csv('dop_files/fillna_999.csv', index=False, header=False)  #############################   \n",
    "        dataset[numeric_features] = dataset[numeric_features].fillna(-99999)\n",
    "        \n",
    "        # заполнение пропусков для категор. приз-в на \"NULL\"\n",
    "        a = [i for i in cat_features if dataset[i].count() != dataset.shape[0]]        #############################\n",
    "        pd.DataFrame(a).to_csv('dop_files/fillna_NULL.csv', index=False, header=False) #############################\n",
    "        dataset[cat_features].fillna('NULL', inplace=True)\n",
    "        \n",
    "        # преобразование полей строкового типа\n",
    "        le = LabelEncoder()\n",
    "        pd.DataFrame(cat_features).to_csv('dop_files/labelEncoding.csv', index=False, header=False) #############################\n",
    "        for i in cat_features:\n",
    "            dataset[i] = dataset[i].astype(str)\n",
    "            dataset[i] = le.fit_transform(dataset[i])\n",
    "            dataset[i] = dataset[i].astype(int)\n",
    "    else:\n",
    "        dataset_copy = dataset.copy()\n",
    "        # преобразование полей строкового типа\n",
    "        le = LabelEncoder()\n",
    "        for i in cat_features:\n",
    "            dataset[i] = dataset[i].astype(str)\n",
    "            dataset[i] = le.fit_transform(dataset[i])\n",
    "            dataset[i] = dataset[i].astype(int)\n",
    "        # оставление пустых ячеек (без заполнения)\n",
    "        s = 1\n",
    "        for j in cat_features:\n",
    "            print(str(s)+'/'+str(len(cat_features)))\n",
    "            s+=1\n",
    "            for i in dataset.index:\n",
    "                if dataset_copy.loc[[i],j].isnull().values[0]: dataset.loc[i,j] = None   \n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "#удаление признаков, имеющих высокую корреляцию с др. пр-ми\n",
    "def corr_drop(X,y):\n",
    "    dataset = X\n",
    "    dataset['y'] = y\n",
    "    corr = dataset.fillna(0).corr() #!!!!! fillna(0)\n",
    "    corr.to_csv('dop_files/corr_1.csv')  #############################\n",
    "    on_drop = ['y']\n",
    "    for i in range(corr.shape[0]-1):\n",
    "        for j in range(i+1,corr.shape[1]-1): #последний столбец - 'y'\n",
    "            if abs(corr.ix[i,j]) > 0.7:\n",
    "                if abs(corr.ix[i,corr.shape[1]-1]) < abs(corr.ix[j,corr.shape[1]-1]): \n",
    "                    on_drop.append(corr.columns[i])\n",
    "                else: \n",
    "                    on_drop.append(corr.columns[j])\n",
    "    pd.DataFrame(on_drop).to_csv('dop_files/drop_corr.csv', index=False, header=False) #############################\n",
    "    dataset.drop(on_drop, axis = 1, inplace = True)\n",
    "    \n",
    "    corr = dataset.fillna(0).corr()       #############################\n",
    "    corr.to_csv('dop_files/corr_2.csv')   #############################\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# возвращает список из N признаков, имеющих наибольшую значимость относительно y, согласно статистике\n",
    "def select_statistic(X, y, count, percent=10):  \n",
    "    select = SelectPercentile(percentile=percent)\n",
    "    select.fit(X.fillna(0), y)\n",
    "    sel = [[i,j] for i,j in zip(X.columns, select.scores_)]\n",
    "    sel = pd.DataFrame( sel, columns=['feature', 'importance'] ).sort_values(by=['importance'], ascending=False)\n",
    "    sel_top = sel.iloc[:60].feature.values\n",
    "    return sel_top\n",
    "\n",
    "\n",
    "#преобразование признаков для моделей лог.регрессии и случайного леса (аналогично, как для бустинга, с небольшими изменениями)\n",
    "def transform_for_rfe_logregr(dataset, cat_features, numeric_features, date_features):\n",
    "    dataset[cat_features].fillna('NULL', inplace=True)\n",
    "    le = LabelEncoder()\n",
    "    on_drop = []\n",
    "    for i in cat_features:\n",
    "        dataset[i] = dataset[i].astype(str)\n",
    "        if dataset[i].describe()[1] <= 2: dataset[i] = le.fit_transform(dataset[i])\n",
    "        elif dataset[i].describe()[1] < 10 or i == 'tb_name':\n",
    "            dataset = pd.get_dummies(dataset, columns=[i])\n",
    "        else: on_drop.append(i)\n",
    "    dataset.drop(on_drop, 1, inplace = True)\n",
    "    today = datetime.date.today()\n",
    "    for i in date_features:  #date_features:\n",
    "        dataset[i] = pd.to_datetime(dataset[i], format='%Y-%m-%d')\n",
    "        dataset[i] = dataset[i].apply(lambda x: ((today - x.date()).days) if x is not None else x)\n",
    "    numeric_features = numeric_features + date_features\n",
    "    dataset[numeric_features] = dataset[numeric_features].fillna(dataset[numeric_features].mean())\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# оценивание information value по всем признакам\n",
    "def calc_iv(df, feature, target, pr=False):\n",
    "    \"\"\"\n",
    "    Set pr=True to enable printing of output.\n",
    "    \n",
    "    Output: \n",
    "      * iv: float,\n",
    "      * data: pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    lst = []\n",
    "\n",
    "    df[feature] = df[feature].fillna('NULL')\n",
    "\n",
    "    for i in range(df[feature].nunique()):\n",
    "        val = list(df[feature].unique())[i]\n",
    "        lst.append([feature,                                                        # Variable\n",
    "                    val,                                                            # Value\n",
    "                    df[df[feature] == val].count()[feature],                        # All\n",
    "                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n",
    "                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]]) # Bad (think: Fraud == 1)\n",
    "\n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n",
    "\n",
    "    data['Share'] = data['All'] / data['All'].sum()\n",
    "    data['Bad Rate'] = data['Bad'] / data['All']\n",
    "    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n",
    "    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n",
    "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
    "\n",
    "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
    "\n",
    "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
    "\n",
    "    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n",
    "    data.index = range(len(data.index))\n",
    "\n",
    "    if pr:\n",
    "        print(data)\n",
    "        print('IV = ', data['IV'].sum())\n",
    "\n",
    "    iv = data['IV'].sum()\n",
    "\n",
    "    return iv, data\n",
    "\n",
    "def iv_sort(dataset, y):\n",
    "    dataset['target'] = y\n",
    "    IV = []\n",
    "    p = 1\n",
    "    for i in range(len(dataset.columns)):\n",
    "        if i % 5 == 0: \n",
    "            print(' ' + '-'*p + ' ',i)\n",
    "            p+=1\n",
    "        iv, dt = calc_iv(dataset.fillna(0), dataset.columns[i], 'target')\n",
    "        IV.append([dataset.columns[i], iv])\n",
    "    IV = pd.DataFrame(IV, columns=['feature', 'iv'])\n",
    "    IV.sort_values('iv', ascending=False, inplace=True)\n",
    "    IV.index = range(IV.shape[0]) \n",
    "    dataset.drop('target', axis=1, inplace=True)\n",
    "\n",
    "    return IV\n",
    "\n",
    "def _check_param_value(parameters: dict) -> dict:\n",
    "    \"\"\"проверка значений гиперпараметров на целочисленность\"\"\"\n",
    "    \n",
    "    int_params = [\n",
    "        \"max_depth\",\n",
    "        \"num_leaves\",\n",
    "        \"min_data_in_leaf\",\n",
    "        \"subsample_for_bin\",\n",
    "        \"n_estimators\",\n",
    "    ]\n",
    "\n",
    "    for param_name in int_params:\n",
    "        if parameters.get(param_name, None):\n",
    "            parameters[param_name] = int(parameters[param_name])\n",
    "    return parameters\n",
    "\n",
    "def objective(parameters):\n",
    "    \"\"\"целевая функция для настройки гиперпараметров\"\"\"\n",
    "    train_set = lgb.Dataset(X_train, y_train)\n",
    "    \n",
    "    parameters = _check_param_value(parameters)\n",
    "    \n",
    "    cv_results = lgb.cv(parameters, train_set, nfold=10, num_boost_round=10000, early_stopping_rounds = 100, metrics='auc', seed=50)\n",
    "    best_score=max(cv_results['auc-mean'])\n",
    "    loss = 1-best_score\n",
    "    return {'loss': loss,\n",
    "            'params':parameters,\n",
    "            'status':STATUS_OK\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------удаление коррелирующих признаков---------\n",
    "def corr_drop(X,y):\n",
    "    dataset = X\n",
    "    dataset['y'] = y\n",
    "    corr = dataset.corr().abs() #!!!!! fillna(0)\n",
    "    on_drop = ['y']\n",
    "\n",
    "    for i in tqdm( range(corr.shape[0]-1) ):\n",
    "        bar.update(i+1)\n",
    "        for j in range(i+1,corr.shape[1]-1): #последний столбец - 'y'\n",
    "            if corr.ix[i,j] > 0.7:\n",
    "                if corr.ix[i,corr.shape[1]-1] < corr.ix[j,corr.shape[1]-1]: \n",
    "                    on_drop.append(corr.columns[i])\n",
    "                else: \n",
    "                    on_drop.append(corr.columns[j])\n",
    "\n",
    "    return list( set(dataset.columns) - set(on_drop) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3 (ZNO0059623792)",
   "language": "python",
   "name": "python35_zno0059623792"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
